\SweaveOpts{results=verbatim,keep.source=TRUE,include=FALSE,eps=FALSE}

\documentclass[12pt]{article}
% \usepackage{mathptmx}
\usepackage[super,comma,sort&compress]{natbib}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[font=it,labelfont=normalfont]{caption}

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathpazo}
\usepackage{authblk}
\usepackage[nolists]{endfloat}
\usepackage{xspace}
\usepackage[noae]{Sweave}
%\usepackage{geometry}
% \geometry{left=1in, right=1in, top=1in, bottom=1in}
% \newcommand{\Rexpr}[1]{\S expr{format(#1,digits=2)}}

\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl,fontsize=\footnotesize,baselinestretch=0.88}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\footnotesize,baselinestretch=0.88}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl,fontsize=\footnotesize,baselinestretch=0.88}

\newcommand{\V}{\operatorname{V}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\R}{\textsf{R}\@\xspace}
\newcommand{\pkg}[1]{\texttt{#1}\@\xspace}
\newcommand{\code}[1]{\texttt{#1}\@\xspace}
\newcommand{\median}{\operatorname{median}}
\newcommand{\ie}{\emph{i.e.,}\@\xspace}
\newcommand{\eg}{\emph{e.g,}\@\xspace}
\newcommand{\sd}{s.d.}
\newcommand{\bG}{\bold{G}}
\newcommand{\bY}{\bold{Y}}

\renewcommand{\topfraction}{.9}
\renewcommand{\bottomfraction}{.9}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\dbltopfraction}{.9}
\renewcommand{\dblfloatpagefraction}{.9}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}

\makeatletter 
\renewcommand\@biblabel[1]{#1} 
\makeatother

\title{Statistical Models for Assessing Agreement in Method Comparison
Studies with Replicate Measurements and Random Raters}
\author[1]{Claus Thorn Ekstr\o m}
\author[2]{Bendix Carstensen}
   
\affil[1]{Biostatistics, Department of Public Health, University of
  Copenhagen, Denmark}
\affil[2]{Steno Diabetes Center, Denmark}
\renewcommand\Authands{ and }


%\Keywords{agreement; method comparison; random raters; mixed models, \R}
%\Plainkeywords{agreement; method comparison; random raters; mixed models,R} %% without formatting
%% at least one keyword must be supplied



%% need no \usepackage{Sweave.sty}



\setkeys{Gin}{width=0.8\textwidth}
\begin{document}

\maketitle

\begin{abstract}
  Agreement between methods for quantitative measurements are
  typically assessed by computing limits of agreement between the
  methods and/or by illustration through Bland-Altman plots.  We
  consider the situation where the observed measurement methods are
  considered a random sample from a population of possible methods,
  and discuss how the underlying linear mixed effects model can be
  extended to this situation. This is relevant when, for example, the
  methods represent raters/judges that are used to score specific
  individuals or items.

 In the case of random methods, we are not interested in estimates
 pertaining to the specific methods, but consider the variation
 between the actually involved methods (\ie raters) as an extra
 source of variation to take into account when generalizing to actual
 clinical performance of a method. In the model we allow raters to
 have individual precision/skill and allow for linked replicates (\ie
 when the order of the items is important).

 Estimation of limits of agreement are shown for a dataset of spatial
 perception of humans. The models are implemented in the
 \pkg{MethComp} package for \R.
\end{abstract}

Keywords: agreement, method comparison, random raters, mixed models,
limits of agreement.

<<echo=FALSE>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
@ 

\section{Introduction}
Comparison of methods for quantitative data is concerned with how well
two methods agree on the measurement of an item. The interest is not
on testing a hypothesis that the mean of the the two methods are
identical, but on estimating the size and the standard deviation of
their differences.

The use of mixed effect models for computing the prediction limits
(often termed the \emph{limits of agreement}) for designs with
replicate measurements on each item have previously been
considered \cite{Carstensen.2008b}. This approach provides a full
statistical model that can compute the limits of agreement in the
presence of replicate measurements instead of relying on summary
measures. Moreover, it forces the investigator to focus on the nature
of replicate measurements: are they exchangeable within each method by
item stratum or only within items (\ie the replicates are linked).

Here we consider the situation where we are not interested in
determining the bias and agreement between two \emph{specific} methods
but are interested in the agreement between two random methods. Thus,
the methods included in the study are regarded as a random sample of
possible methods from a larger population of available methods; random
methods are relevant, for example, when a group of judges/raters are
asked to rate a set of items using a predetermined scale. This is a
common occurrence for example when medical doctors are asked to give
second or third opinions on measurements taken on patients. We are
interested in how well medical doctors in general agree on the scoring
of a particular condition, more so than determining how well specific
doctors compare to each other.

The focus on random raters (as opposed to fixed methods/raters) is not
new and is a often found in relation to the calculation of intraclass
correlation coefficients (ICC) for random factorial design
\cite{Shoukri2010,Kilem2012}. However, they are typically concerned
with estimating the ratio of the intraclass variation to the total
variance. Here, our focus is on limits of agreement and we present a
model that easily accommodates individual precision/skill for each
rater and handles unbalanced designs where raters may rate different
subsets of items.


Section \ref{sec:model} sets up the necessary mixed effect model that
accommodates random raters, and Section \ref{sec:replicate} extends
the model to the situation where replicate measurements from the
methods are available on each item. Section \ref{sec:repeatability}
discusses repeatability while the model is applied to a real dataset
with random raters in Section \ref{sec:example}.

\section{Models for agreement among random raters}
\label{sec:model}

The classical setup for comparison of measurements methods is one
where exactly one measurement is taken with each method on each item
(here, item can refer to, for example, an individual, a sample, or an
image). The limits of agreement for two methods are computed as the
prediction interval for the difference between future measurements
taken by the two methods on a new item or, equivalently as a
prediction interval for a measurement on an item by method B, given a
measurement by method A. The focus of such studies is typically the
comparison (and possibly prediction) between a few specific methods of
interest.

When multiple raters are compared, the raters play the role of
methods, but we are not interested in the difference between any two
specific raters. Instead we consider each rater as a ``random'' rater
from an (essentially infinite) population of raters. Thus, when we
compute the prediction limits for the difference between measurements
by two randomly chosen raters we should ensure that the variation
between the random raters is taken into account in the modeling and
that the variation is included in the computation of the prediction
interval.

If we carry over the two-way analysis of variance model from the
classical Bland-Altman set-up
\cite{Bland:Altman.1999,Carstensen.2008b} we model the value for a
measurement taken on item $i$ by rater $m$ as:
\begin{equation} \label{mod1}
\begin{split}
Y_{mi} &= \mu_i + b_m + e_{mi}, \\
b_m &~ \sim N(0, \xi^2), \\
e_{mi} &~ \sim N(0, \sigma_m^2),
\end{split}
\end{equation}
where $\mu_i$ is the ``true'' value for item $i$, $b_m$ is a random
effect that models a random bias for measurements taken by rater $m$,
$\xi$ is the variation of biases between raters, and $\sigma_m$ is
the individual variation of rater $m$.

The ``true'' value for item $i$ is of course arbitrary; a constant may
be added to all $\mu_i$s provided it is subtracted from the $b_m$s;
but since we have constrained the mean of the $b_m$s to be 0, the
$\mu_i$s represent the average assessment of item $i$ by the set of
raters at hand. Note that since the raters (or more precisely, the
variation between them) is the focus of interest, the $\mu_i$s are
essentially nuisance parameters.

Note that each rater is allowed to have his own precision such that
some raters can be very precise (\ie have low residual variation)
while others can be less precise.

The limits of agreement for the difference between measurements taken
by two random raters ($m$ and $m'$) on the same item corresponds to
the prediction interval of the difference $Y_{mi} - Y_{m'i}$.  If
measurements by different raters are assumed to be independent we get
that the limits of agreement (prediction interval) for the
difference between the two raters on a new item is
\begin{equation}
0 \pm z \sqrt{\V(Y_{mi} - Y_{m'i})} = 0 \pm z \sqrt{\V(Y_{mi}) + \V(Y_{m'i})},
\end{equation}
where $z$ is the quantile corresponding to the desired level of the
prediction interval. Under model \eqref{mod1}, the 95\% limits of
agreement simplifies to
\begin{equation}
  0 \pm 1.96 \times \sqrt{2\times\xi^2 + \sigma_m^2 + \sigma_{m'}^2}. \label{eq:LoA1}
\end{equation}
The value 1.96 can of course be replaced by a suitable quantile from
the t distribution; the normal convention is to use 2 (which
incidentally is the 97.5\% quantile of the t distribution with 60
degrees of freedom). In the rest of this paper we shall use 2 for
convenience.


Even without replicate measurements it is possible to estimate the
individual residual variance for each rater, because model
\eqref{mod1} by the very nature of the randomness of raters must
impose an assumption of 0 average difference between raters. This
means that the estimate of the single rater's variation is strongly
dependent on the other raters' results, because it is essentially the
variation around the common means ($\mu_i$).
%
It also emphasizes the crucial nature of the assumptions behind
\emph{random} raters, a truly random sample of raters is required to
ensure that the empirical means across the raters at hand is a fair
approximation of the true population mean. Or put the another way
round, the generalizations are only valid to a population of raters
of which those in the sample at hand can be considered a random
sample. And predictions of precision of future ratings also require
that the sample for which the prediction is made is random too.

% {\footnotesize
% The model \eqref{mod1} is a two-way analysis of variance
% model with random row (\ie \texttt{meth}-) effect. The simple version
% with identical residual variances between methods can be fitted with
% either \texttt{lmer} or \texttt{lme}, whereas there is no facility for
% stratum-specific variances implemented in \texttt{lmer}, \texttt{lme}
% must be used:
% <<>>=
% library( MethComp )
% data( Ancona )
% Ancona <- Meth( Ancona, 1,2,3,4 )
% library(lme4)
% print( lmer( y ~ item - 1 + (1|meth),
%              data=subset(Ancona,repl==1) ),
%        correlation=FALSE )
% print( lme( y ~ item - 1, random = ~ 1|meth,
%             data=subset(Ancona,repl==1) ) )
% print( lme( y ~item-1, random = ~1|meth,
%             weights = varIdent( form=~1|meth ),
%             data=subset(Ancona,repl==1) ) )
% @
% }

When we wish to compare the agreement between two \emph{random} raters
we need to take the variation in the individual rater precisions into
account since we do not know which particular two raters we will
use. The variance for rater $m$ on item $i$ is
\begin{eqnarray*}
\V(Y_{mi}) & = & \xi^2 + \sigma_m^2
\end{eqnarray*}
where we let the individual residual variances $\sigma_m^2$
follow some distribution of the residual variances which has support on the positive numbers.
%to account for the heterogeneity of variances between raters.
Thus, the distribution of the residual variances is central for the
prediction of differences between rates; it represents the
distribution of ``skills'' among available raters.  If prior knowledge
about the precision distribution is available then that information
can be used to model the distribution of the residual variances.  In the following we assume that we have no
prior knowledge about the precision and/or that we have too little
information to be able to verify any distributional assumptions about
the residual variances that we might have.

The law of total variance provides the variance of a measurement from
a randomly chosen rater from the population when applied to a fixed
item:
\begin{equation}
\begin{split}
\V(Y_{mi}) &=  \V_\sigma\bigl(\E(Y_{mi}|\sigma_m^2)\bigr) +
              \E_\sigma\bigl(\V(Y_{mi}|\sigma_m^2)\bigr) \\
          &= \V_\sigma( \mu_i ) + \E_\sigma( \xi^2 + \sigma_m^2)\\
          &= \xi^2 + \E_\sigma(\sigma_m^2).
          \label{tot-var}
\end{split}
\end{equation}
Even if only one measurement per method (rater) and item is available
it is possible to estimate all parameters in model \eqref{mod1}, so in
order to produce limits of agreement for random raters in this simple
setup we just need to estimate the mean variation among raters.

%Now, the outer variances and expectations might as well be computed under
%the distribution of $\sigma_m^2$ as under $\sigma_m$.
%So by this token, the empirical counterpart of this results can be either of:
To this end we use the empirical counterpart of \eqref{tot-var}:
%The empirical counterpart of \eqref{tot-var} is
\begin{equation}
  \begin{split}
   \widehat{\xi^2} + \widehat{\E(\sigma_m^2)}
   &\approx \widehat{\xi^2} + \frac 1M \sum_{m=1}^M \widehat{\sigma_m^2}
  % \\
 %  &\approx \hat{\xi}^2 + \left(\frac 1M \sum_{m=1}^M \hat{\sigma}_m\right)^2\\
   \label{approximation}
  \end{split}
\end{equation}
where $M$ is the number of raters available in the dataset.  Hence,
the estimate for the 95\% limits of agreement between two randomly
chosen raters becomes
\begin{equation}
  0 \pm 2 \times \sqrt{2\times\bigl( \widehat{\xi^2} +
      \frac 1M \sum_{m=1}^M \widehat{\sigma_m^2}
%      \median(\hat{\sigma}_m^2)
      \bigr)}
\end{equation}

Equation~\ref{tot-var} states that the variance of a measurement by a
randomly chosen rater is the sum of between rater variance and the
average within-rater variance. The estimate relies heavily on the
assumption that the sample of raters at hand is representative of the
population of future raters. Moreover, the within-rater variation is
poorly determined if the number of raters in the study is small.

\section{Model for replicate measurements}
\label{sec:replicate}
As mentioned above, even if raters are measuring each item only once,
we can still estimate the residual variation of each rater; it will
simply be the variation around the common item-means. This is where
the situation with random raters differs from comparing specific
measurement methods. Because raters are considered random, we must
necessarily assume that the mean difference between two raters is 0,
and by that token that any deviation from 0 is random.  This means
that the implicit assumption of randomly chosen raters is heavily
exploited in the case without replicate measurements.

If we really want to assess the variability of the \emph{precision} of
the raters it is mandatory to have replicate measurements of each
rater (``method'') by each item; in that case observations are
classified by replicate too, so we need a more elaborate
model. However, replicates come in two guises: linked and
exchangeable \cite{Carstensen.2008b}.

Replicates are \emph{linked} if the first replicate by all raters are
made at the same time (or under similar circumstances), and if the
second replicate by all raters are made at the same time too,
etc. This means that the \emph{numbering} (but not necessarily the
\emph{ordering}) of the replicates carries some information about
similar circumstance of the measurement.  Replicates are called
\emph{exchangeable} if this is not the case, that is if replicate
numbering can be freely exchanged within a given combination of
$(m,i)$.

This distinction has implications for the models used to describe data
and to derive the relevant formulas for the predictions.

\subsection{Linked replicates}
If replicates are linked, we use the following extension of the simple
model \eqref{mod1} for the measurement $y_{mir}$ by rater $m$, on item
$i$, replicate $r$:
\begin{equation}
\begin{split}
Y_{mir} &= \mu_i + b_m + a_{ir} + c_{mi} + e_{mir}, \\
b_m &~ \sim N(0, \xi^2) \\
a_{ir} &~ \sim N(0, \omega^2) \\
c_{mi} &~ \sim N(0, \tau_m^2) \\
e_{mir} &~ \sim N(0, \sigma_m^2)\\ \label{mod-linked}
\end{split}
\end{equation}
Note that two new variance components have been added relative to the
simple model in Equation~\ref{mod1}.

The first, $\omega$, is the variation between replication instances;
as such it is in principle irrelevant for the comparison of raters.
The second, $\tau_m$, is the variation between items within each
method --- a method-specific interaction with the items. It represents
the variability of a rater across items; that is how a specific
rater's measurements varies relative to the \emph{average} measurement
by all raters on a given item. Thus, this is a variance component
whose size for the individual rater is very strongly tied to the
concept of randomly chosen raters, in the sense that the estimates of
$\tau_m$ will depend on the sample of raters to a much larger degree
than will the estimates of $\sigma_m$ which are entirely estimates
of the individual rater's variation around his own measurements.

Model \eqref{mod-linked} is a standard variance component model as is
the model defined by Equation~\ref{mod1}.  When we derive the limits of
agreement in this case we use the same calculations as in the simple
case; but note that the measurements by different methods are no more
independent because of the term $a_{ir}$. Incidentally, these terms
cancel out when computing $\V(Y_{mir}-Y_{m'ir})$, because indices $i$ and
$r$ are identical for the two terms.

By the same arguments that lead to \eqref{eq:LoA1}, we get limits of
agreement between two randomly chosen raters as:
\[
  0 \pm 2 \times \sqrt{2\times\xi^2 + \tau_m^2 + \tau_{m'}^2 + \sigma_m^2 + \sigma_{m'}^2}.
\]
Thus, when considering random raters we will not only need to estimate
the average residual variation between raters, but also the average
item-by-rater variation between raters; which leads to limits of
agreement estimated as:
\begin{equation}
  \begin{split}
   0 \pm 2 & \times \sqrt{2\times\left( \hat{\xi}^2 +
      \frac 1M \sum_{m=1}^M (\hat{\tau}_m^2 + \hat{\sigma}_m^2)
                     \right)}
%  \\
%   0 \pm 2 & \times \sqrt{2\times\left( \hat{\xi}^2
%                       + \median( \hat{\tau}_m^2 ) +
%                        \median(\hat{\sigma}_m^2)\right)} \\
  \end{split}.
  \label{eq:LoA-linked}
\end{equation}

\subsection{Exchangeable replicates}
When we have exchangeable replicates, we use a model very similar to
the one used above, except that the variance component $\omega$
(corresponding to the random effects $a_{ir}$) is absent. This has no
effect on the formula for the variation of the differences, so formula
\eqref{eq:LoA-linked} above for the limits of agreement will be the
same for the case of exchangeable replicates as it was for linked
replicates. However, note that the model fitted to obtain the relevant
estimates is different for exchangeable replicates compared to linked
replicates.

\section{Repeatability}
\label{sec:repeatability}

The limits of agreement are not always the only issue of interest ---
the assessment of method specific repeatability and reproducibility
may be of interest in their own right. In particular, if a method has
large variation between replicates on the same item then the
repeatability and agreement with other methods will be poor.
Repeatability can only be assessed when replicate measurements by each
method are available.

In classical assessment of specific measurement methods, the
repeatability coefficient for a method is defined as the upper limit
of a prediction interval for the absolute difference between two
measurements by the same method on the same item under identical
circumstances.
% A small repeatability coefficient corresponds to good
% repeatability (\ie, little variation among replicates) while a large
% repeatability coefficient corresponds to poor repeatability.

If the standard deviation of a measurement is $\sigma$ then the
repeatability coefficient is $2 \times \sqrt{2\sigma^2} \approx 2.8
\sigma$.

In the case of random raters, the repeatability coefficient has a
slightly different meaning because we cannot hinge it on estimates of
variances from any specific rater --- they will just be a random set
of variances. Instead the repeatability must refer to the
average/expected repeatability, or even, if we cling to the classical
definition, the variability of the repeatability as we may expect to
see it in a sample of raters.

When replicates are exchangeable, the difference between two replicate
measurements, $r$ and $r'$ taken by rater $m$ on item $i$ is
\begin{equation}
 Y_{mir} - Y_{mir'} = e_{mir}- e_{mir'}
\end{equation}
so the repeatability is based only on the residual standard deviation,
\ie $2.8\sigma_m$. For linked replicates this difference becomes
\begin{equation}
 Y_{mir} - Y_{mir'} = a_{ir}- a_{ir'}  + e_{mir}- e_{mir'}, \label{eq:linked-repeat}
\end{equation}
and the variation between replicates taken on the same item should be
factored into the calculation of the repeatability coefficient which
then becomes $2.8\sqrt{\omega^2 +\sigma_m^2}$.

The latter argument assumes that the variability between replication
occasions can be considered representative of future scenarios.  If the
replicates are taken under substantially different circumstances, then
the variance component $\omega$ may be considered irrelevant for the
repeatability and the repeatability coefficient should be based on the
measurement errors alone, \ie use $2.8\sigma_m$.  However, if indeed
the replicates are taken under substantially different circumstances
it may be argued that we are not really measuring the same item
repeatedly. Instead the effects of differing replication circumstances
could be modeled by a systematic effect. Hence there is no
subject-matter-free way of defining repeatability from the variance
components in the models.

%Thus, repeatability varies between raters too, and in the case of
%random raters essentially only the median of these makes sense as a
%summary estimate --- half of the times one will have a rater with
%better repeatability, half of the times one with worse.

%\textsf{XXX : Jeg vil egentlig hellere skrive nedenstående. Måske er
% det bare fordi jeg ikke helt forstår argumentet ovenfor. Hvis vi
% vælger helt at fjerne medianen skal vi også huske at rette koden til
% i repeatabilityafsnittet i Ancona.Rnw:
Thus, repeatability varies between raters too, so the mean of the
repeatability coefficients makes sense as a summary estimate (even
though the mean coefficient may not coincide with that of any
particular rater found in the dataset).
%}
% \clearpage

\section{Worked example: spatial perception of point swarms}
\label{sec:example}

Some people have keen spatial perception and are able to almost
instantaneous\-ly give a reasonable guess of, for example, the number of
individuals in a crowd. We collected data on spatial perception from
participants attending a course on comparison of measurement
methods. Thus, we will use attendees of the course as a random
selection of raters and will try to determine how well two random
raters agree on the number of points in a point swarm.

% \input{Ancona}
\SweaveInput{Ancona-smmr}

% \clearpage

\section{Discussion}
%Often it is of interest to assess the similarity between different
%methods of measurement, and when measurements
%are taken with different methods we wish to determine whether the methods
%agree with one another, or rather, quantify the degree of disagreement
%in order to be able to make an informed substantive judgment.

In this paper we have extended the common modeling approach 
% of \citet{Carstensen.2008b} 
to address the question of precision when the
methods represent a random sample of observers (raters) that are asked
to produce a measurement. In a sense this is analogous to the binary
case \cite{fleiss:etal:2004}.  Our extension
incorporates the methods as random effects in the model and in that
vein, the prediction of the difference between measurements by two
random raters on a future item will inevitably be a prediction that
has zero mean, and where only the variation between and within raters
is involved. The proposed modeling approach can handle individual
skills of the raters as well as it accommodates designs where not
every rater necessarily needs to assess every item.

The calculation of the limits of agreement relies on the law of total
variance so it should be noted that the approximations used may be
rather crude when the number of available raters is small.

The empirical mean of rater variances used in \eqref{approximation} is
obviously a good estimator of the theoretical mean variance when the
number of available raters is large. For smaller number of raters one
might be inclined to use a more robust estimator of the central
tendency as for example the median.  However, while the mean and the
median will give the same results if the distribution of rater
variances is symmetric, the two summary measures will differ when the
distribution of the residual variances is asymmetric. Since we seek to
estimate the mean of the distribution of residual variances, we would
introduce a bias in our computations and the bias will persist even if
the number of raters increases.

\SweaveInput{median}

As an example, consider a situation with two types of raters: those
with high precision (small variance) and those with low precision
(large variance) and assume the two types of raters are represented in
the population at a ratio of 3:1, say. Then the median will match the
variance of the most prevalent group and will consistently provide a
wrong estimate of the mean within-rater variance.

This problem is illustrated in Figure~\ref{fig:dists} where we have
simulated data using three different distributions for the residual
variances within raters: a uniform distribution, a skewed distribution and a
bimodal distribution. For each distribution we have looked at
scenarios where an increasing number of raters are randomly selected
from a population, and for each scenario made 500 simulated selections
from the distribution of rater-variances.
%
We have shown the distribution of 500 simulated means and medians of
the rater variances for different sizes of the random sample of raters
(2--40). The distributions are shown using the 2.5, 50 and 97.5\%
quantiles of the simulated means of variances.

The uniform distribution is symmetric and the mean and median produce
similar results. For the skewed distribution we get unbiased estimates
for the mean but biased (albeit more precise) estimates for the
median. For the bimodal distribution we get a consistent bias for the
median, which clearly underestimates the true mean. It is an
\emph{under}estimate because the largest chunk of the variances are
the smaller ones. There are two noticeable drops in the upper limits
of the quantiles for the median estimates for the bimodal distribution
between 8 and 10 raters and between 16 and 18 raters. These marked
drops are artifacts due to the combination of applying the median
together with a bimodal distribution. There are essentially two types
of raters, and when the number of raters is low we may sometimes ---
simply due to random sampling --- obtain the majority of observations
from groups with the largest variance. When the number of raters
increases, the probability of having the majority of raters from the
smallest falls below 2.5\% and the bands stabilizes.

Hence, the mean is still preferable as an estimator to more classical
robust measures like the median. But it should be emphasized that
predictions of differences between future randomly chosen raters is
highly dependent on the assumption that the sample at hand is
actually representative and that there is no way to test this assumption.


% \clearpage

\nocite{R}
%\nocite{Epi}
%\nocite{lattice}
\nocite{MethComp}

\section*{Funding}
This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.


\bibliographystyle{vancouver}
\bibliography{localrefs}

\end{document}
